{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd6bdf0-47ea-4d07-bcf6-83bc9a9e0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350eae0b-d882-4b72-b4ca-49dd4b208462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b2bc43-c81a-4512-930a-d0f7b5bcc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenCLIPEmbedder:\n",
    "    def __init__(self, model_name=\"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    def encode_text(self, texts, normalize=True):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        tokens = self.tokenizer(texts).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.model.encode_text(tokens)\n",
    "        if normalize:\n",
    "            features /= features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy()\n",
    "    \n",
    "    def encode_image(self, images, normalize=True):\n",
    "        if isinstance(images, str):\n",
    "            images = [Image.open(images)]\n",
    "        elif isinstance(images, Image.Image):\n",
    "            images = [images]\n",
    "        \n",
    "        images = torch.stack([self.preprocess(img) for img in images]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.model.encode_image(images)\n",
    "        if normalize:\n",
    "            features /= features.norm(dim=-1, keepdim=True)\n",
    "        return features.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a1faa4-c94a-4447-813f-b243361ed872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafb58a1-1e40-4b7b-9fcf-d746a67cacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(Path(os.getcwd()).resolve().parents[1] , \"local_only\", \"data\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "DATASET_NAME = \"nlphuji/flickr30k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76011a12-0371-4e01-aeeb-33b10e2654bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flickr30k_dataset(dataset_name, download_location=None):\n",
    "    dataset = load_dataset(dataset_name, cache_dir=download_location)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42dd2d9-a593-4b32-aa00-464612ef37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = download_flickr30k_dataset(dataset_name = DATASET_NAME, download_location = CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39031043-434c-49e4-b312-9bfaf4531628",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e3fd54-173d-4685-bcdc-21b6092bf810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=375x500>,\n",
       " 'caption': ['A child in a pink dress is climbing up a set of stairs in an entry way.',\n",
       "  'A little girl in a pink dress going into a wooden cabin.',\n",
       "  'A little girl climbing the stairs to her playhouse.',\n",
       "  'A little girl climbing into a wooden playhouse.',\n",
       "  'A girl going into a wooden building.'],\n",
       " 'sentids': ['10', '11', '12', '13', '14'],\n",
       " 'split': 'train',\n",
       " 'img_id': '2',\n",
       " 'filename': '1000268201.jpg'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eade967-95c9-4670-84c7-840cc497e50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238527e2fe504176b3bf738db2809214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = OpenCLIPEmbedder(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f712d4be-bae1-4ee3-8b87-5512b3b696e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = data[2]\n",
    "image = [record['image']]\n",
    "texts = record['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "539ff00e-3ced-4d5c-acc3-faf95d67ec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way.',\n",
       " 'A little girl in a pink dress going into a wooden cabin.',\n",
       " 'A little girl climbing the stairs to her playhouse.',\n",
       " 'A little girl climbing into a wooden playhouse.',\n",
       " 'A girl going into a wooden building.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b108f6ca-02d6-43df-a6be-7165ebeec2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 ms, sys: 11.4 ms, total: 339 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_embeddings = embed_model.encode_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7484e29c-de57-4020-8885-4493170a914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = embed_model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "569aba4b-3c51-4e59-98f4-07a432c10897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: A little girl climbing into a wooden playhouse. | Similarity: 0.3564\n",
      "Caption: A little girl climbing the stairs to her playhouse. | Similarity: 0.3455\n",
      "Caption: A little girl in a pink dress going into a wooden cabin. | Similarity: 0.3359\n",
      "Caption: A girl going into a wooden building. | Similarity: 0.3236\n",
      "Caption: A child in a pink dress is climbing up a set of stairs in an entry way. | Similarity: 0.2667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure correct shapes\n",
    "image_vec = image_embeddings[0]           # shape: (dim,)\n",
    "text_vecs = text_embeddings              # shape: (N, dim)\n",
    "\n",
    "# Cosine similarity (dot product, since normalized)\n",
    "similarities = np.dot(text_vecs, image_vec)  # shape: (N,)\n",
    "\n",
    "# Rank captions by similarity\n",
    "sorted_indices = np.argsort(similarities)[::-1]\n",
    "for idx in sorted_indices:\n",
    "    print(f\"Caption: {texts[idx]} | Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22623ebd-5919-47a6-9d7a-9d0d7e5abf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption_similarities(image_embeddings, text_embeddings):\n",
    "        # Ensure correct shapes\n",
    "    image_vec = image_embeddings[0]           # shape: (dim,)\n",
    "    text_vecs = text_embeddings              # shape: (N, dim)\n",
    "    \n",
    "    # Cosine similarity (dot product, since normalized)\n",
    "    similarities = np.dot(text_vecs, image_vec)  # shape: (N,)\n",
    "    \n",
    "    # Rank captions by similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        print(f\"Caption: {texts[idx]} | Similarity: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c163e5b-5e42-4f1c-b521-5cdf19c5a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard. | Similarity: 0.3397\n",
      "Caption: Two young, White males are outside near many bushes. | Similarity: 0.2736\n",
      "Caption: Two men in green shirts are standing in a yard. | Similarity: 0.2658\n",
      "Caption: Two friends enjoy time spent together. | Similarity: 0.2272\n",
      "Caption: A man in a blue shirt standing in a garden. | Similarity: 0.1979\n",
      "=====\n",
      "Caption: Several men in hard hats are operating a giant pulley system. | Similarity: 0.3361\n",
      "Caption: Workers look down from up above on a piece of equipment. | Similarity: 0.3074\n",
      "Caption: Three men on a large rig. | Similarity: 0.2631\n",
      "Caption: Four men on top of a tall structure. | Similarity: 0.2566\n",
      "Caption: Two men working on a machine wearing hard hats. | Similarity: 0.2100\n",
      "=====\n",
      "Caption: A little girl climbing into a wooden playhouse. | Similarity: 0.3564\n",
      "Caption: A little girl climbing the stairs to her playhouse. | Similarity: 0.3455\n",
      "Caption: A little girl in a pink dress going into a wooden cabin. | Similarity: 0.3359\n",
      "Caption: A girl going into a wooden building. | Similarity: 0.3236\n",
      "Caption: A child in a pink dress is climbing up a set of stairs in an entry way. | Similarity: 0.2667\n",
      "=====\n",
      "Caption: Man in blue shirt and jeans on ladder cleaning windows | Similarity: 0.3327\n",
      "Caption: A man on a ladder cleans a window | Similarity: 0.3273\n",
      "Caption: A man in a blue shirt is standing on a ladder cleaning a window. | Similarity: 0.3230\n",
      "Caption: A man on a ladder cleans the window of a tall building. | Similarity: 0.3042\n",
      "Caption: Someone in a blue shirt and hat is standing on stair and leaning against a window. | Similarity: 0.2417\n",
      "=====\n",
      "Caption: Two guy cooking and joking around with the camera. | Similarity: 0.3888\n",
      "Caption: Two men are cooking a meal. | Similarity: 0.3330\n",
      "Caption: Two men in a kitchen cooking food on a stove. | Similarity: 0.3320\n",
      "Caption: Two men are at the stove preparing food. | Similarity: 0.3064\n",
      "Caption: Two men, one in a gray shirt, one in a black shirt, standing near a stove. | Similarity: 0.2947\n",
      "=====\n",
      "Caption: A man is fixing the guitar players costume. | Similarity: 0.2996\n",
      "Caption: A man in green holds a guitar while the other man observes his shirt. | Similarity: 0.2862\n",
      "Caption: Two people in the photo are playing the guitar and the other is poking at him. | Similarity: 0.2558\n",
      "Caption: The two boys playing guitar | Similarity: 0.1956\n",
      "Caption: A guy stitching up another man's coat. | Similarity: 0.1932\n",
      "=====\n",
      "Caption: A man completes the finishing touches on a stuffed lion. | Similarity: 0.3941\n",
      "Caption: A man holds a large stuffed lion toy. | Similarity: 0.3630\n",
      "Caption: A man is sitting on a chair holding a large stuffed animal. | Similarity: 0.3620\n",
      "Caption: A man is smiling at a stuffed lion | Similarity: 0.3534\n",
      "Caption: A man sits in a chair while holding a large stuffed animal of a lion. | Similarity: 0.3331\n",
      "=====\n",
      "Caption: A girl is on rollerskates talking on her cellphone standing in a parking lot. | Similarity: 0.2856\n",
      "Caption: Woman talking on cellphone and wearing rollerskates. | Similarity: 0.2822\n",
      "Caption: A young adult wearing rollerblades, holding a cellular phone to her ear. | Similarity: 0.2702\n",
      "Caption: There is a young girl on her cellphone while skating. | Similarity: 0.2591\n",
      "Caption: A trendy girl talking on her cellphone while gliding slowly down the street. | Similarity: 0.2519\n",
      "=====\n",
      "Caption: A young woman walks past two young people dressed in hip black outfits. | Similarity: 0.2997\n",
      "Caption: Several people standing outside a building. | Similarity: 0.2594\n",
      "Caption: Three people are standing outside near large pipes and a metal railing. | Similarity: 0.2501\n",
      "Caption: An asian man wearing a black suit stands near a dark-haired woman and a brown-haired woman. | Similarity: 0.2165\n",
      "Caption: A woman with a large purse is walking by a gate. | Similarity: 0.2028\n",
      "=====\n",
      "Caption: Two youths are jumping over a roadside railing, at night. | Similarity: 0.3869\n",
      "Caption: Two men in Germany jumping over a rail at the same time without shirts. | Similarity: 0.3862\n",
      "Caption: Two men with no shirts jumping over a rail. | Similarity: 0.3491\n",
      "Caption: Two guys jumping over a gate together | Similarity: 0.3069\n",
      "Caption: Boys dancing on poles in the middle of the night. | Similarity: 0.3048\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    data_point = data[i]\n",
    "    image = [data_point['image']]\n",
    "    texts = data_point['caption']\n",
    "\n",
    "    text_embeddings = embed_model.encode_text(texts)\n",
    "    image_embeddings = embed_model.encode_image(image)\n",
    "\n",
    "    get_image_caption_similarities(image_embeddings, text_embeddings)\n",
    "\n",
    "    print(5*'=')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
